{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangChain](../assets/langchain.svg)\n",
    "\n",
    "# LangChain Demo\n",
    "\n",
    "Podríamos decir que es el marco de trabajo o _framework_ con la comunidad más grande en estos momentos. Ofrece capacidades de integración con LLMs en varios lenguajes de programación, siendo el principal Python y JavaScript, y existiendo versiones para otros lenguajes como C#.\n",
    "\n",
    "Al ser un proyecto comunitario, a veces adolece de mucho caos entre las versiones, aunque últimamente lo están haciendo mejor desde la salida de `LangGraph` (la parte de pago).\n",
    "\n",
    "## Ejemplo del taller\n",
    "\n",
    "Vamos a realizar un ejemplo de cómo diseñar e implementar un chatbot impulsado por un modelo de lenguaje grande (LLM, _Large Language Model_), en este caso de Azure OpenAI. Este chatbot podrá mantener una conversación y recordar interacciones previas con un modelo de chat.\n",
    "\n",
    "Hay que tomar en cuenta que este chatbot que construiremos solo utilizará el modelo de lenguaje para conversar. Más adelante en el taller trataremos otros conceptos relacionados tales como:\n",
    "\n",
    " - Generación Aumentada por Recuperación: Que permite una experiencia de chatbot sobre una fuente de datos externa, lo cual es conocido habitualmente por sus siglas en ingles de RAG (_Retrieval Augmented Generation_).\n",
    " - Agentes: Construir un chatbot que pueda tomar acciones, algunas veces de forma independiente, para satisfacer un requerimiento específico en un área de dominio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencias y Referencias\n",
    "\n",
    "Primero instalaremos las siguientes dependencias:\n",
    "\n",
    " - `langchain-core` → contiene las abstracciones base que impulsan el resto del ecosistema de LangChain. Estas abstracciones están diseñadas para ser modulares y simples, permitiendo que cualquier proveedor implemente la interfaz requerida y se integre fácilmente con el resto del ecosistema.\n",
    " - `langchain_openai` → proporciona las integraciones de LangChain para OpenAI a través de su SDK, tanto de la propia empresa detrás de ChatGPT como la versión de Azure OpenAI (o cualquier otra).\n",
    " - `langgraph` → es una biblioteca o librería de LangChain diseñada para construir aplicaciones con múltiples actores y estado utilizando modelos de lenguaje grande (LLMs).\n",
    " - `python-dotenv` → es una biblioteca de Python que facilita la gestión de variables de entorno en tus proyectos.\n",
    "\n",
    "Con estas dependencias instaladas, importamos los siguienbtes recursos:\n",
    "\n",
    "- `import os` → Esta es una biblioteca estándar de Python que proporciona una forma de interactuar con el sistema operativo.\n",
    "- `from dotenv import load_dotenv` → Importa la capacidad de leer las variables de entorno desde un archivo `.env`. La función `load_dotenv` lee el archivo `.env` y carga las variables definidas en él como variables de entorno del sistema. Esto es útil para gestionar configuraciones sensibles como claves API y credenciales sin hardcodearlas en el código fuente.\n",
    "- `from langchain_openai import AzureChatOpenAI` → Importa la clase `AzureChatOpenAI` que permite interactuar con los modelos de chat de OpenAI alojados en Azure. Esta clase facilita la configuración y el uso de estos modelos para generar respuestas de chat.\n",
    "\n",
    "Finalmente leemos las variables de entorno que nos proporcionan los siguientes valores:\n",
    "\n",
    "- `AZURE_OPENAI_ENDPOINT` → Es la dirección (o URL) desde la cual podemos acceder al servicio de Azure OpenAI que tengamos desplegado en nuestra instancia de la nube de Microsoft.\n",
    "- `AZURE_OPENAI_DEPLOYMENT_NAME` → Es el nombre del despliegue del modelo (LLM) que vamos a utilizar.\n",
    "- `AZURE_OPENAI_MODEL_NAME` → Es el nombre (o tipo) del modelo que vamos a utilizar. Por ejemplo `gpt-4o`.\n",
    "- `AZURE_OPENAI_API_VERSION` → Es la versión del API de Azure con la cual estarmos interactuando con el servicio. Por ejemplo `2024-02-15-preview`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain-core in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain_openai in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.2.9)\n",
      "Requirement already satisfied: langgraph in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.2.53)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core) (0.1.145)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core) (2.10.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_openai) (1.55.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_openai) (0.8.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.4 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langgraph) (2.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.32 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langgraph) (0.1.36)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.27.2)\n",
      "Requirement already satisfied: httpx-sse>=0.4.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.4.0)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langgraph-sdk<0.2.0,>=0.1.32->langgraph) (3.10.12)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.7.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.67.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.27.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain_openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.32->langgraph) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rliberoff.pharus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>4->openai<2.0.0,>=1.54.0->langchain_openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-core langchain_openai langgraph python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the model\n",
    "model = AzureChatOpenAI(    \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    model_name=os.environ[\"AZURE_OPENAI_MODEL_NAME\"],\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, usemos el modelo directamente. Los `ChatModels` son instancias de los 'Runnables' de LangChain, lo que significa que exponen una interfaz estándar para interactuar con ellos. Para simplemente llamar al modelo, podemos pasar una lista de mensajes al método `.invoke`, el cual es un método que de forma síncrona se quedará esperando por al respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_04751d0b65', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-76b11e68-cbae-4c5c-9aad-9e8e8a79d746-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo por sí solo no tiene ningún concepto de estado. Por ejemplo, si haces una pregunta de seguimiento no sabrá de qué le estás hablando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry, but I can't determine your name based on the information provided.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 11, 'total_tokens': 26, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_04751d0b65', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-80c4c75e-373d-4552-a6db-f7b468ffef2a-0', usage_metadata={'input_tokens': 11, 'output_tokens': 15, 'total_tokens': 26, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para solucionar esto, necesitamos pasar todo el historial de la conversación al modelo. Veamos qué sucede cuando hacemos eso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You mentioned that your name is Bob. How can I help you today, Bob?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 33, 'total_tokens': 50, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_04751d0b65', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-eb19c36b-9aa3-4c85-8d91-8543fb99d6a3-0', usage_metadata={'input_tokens': 33, 'output_tokens': 17, 'total_tokens': 50, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Y ahora podemos ver que obtenemos una buena respuesta!\n",
    "\n",
    "Esta es la idea básica que sustenta la capacidad de un chatbot para interactuar de manera conversacional. Entonces, ¿cómo implementamos esto de la mejor manera?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historial de la Conversación\n",
    "\n",
    "El componente LangGraph de LangChain implementa una capa de persistencia incorporada, lo que lo hace ideal para aplicaciones de chat que soportan múltiples turnos conversacionales.\n",
    "\n",
    "Envolver nuestro modelo de chat en una aplicación mínima de LangGraph nos permite persistir automáticamente el historial de mensajes, simplificando el desarrollo de aplicaciones de múltiples turnos.\n",
    "\n",
    "LangGraph viene con un simple punto de control en memoria, que usamos a continuación. En un entorno real o productivo lo más conveniente es usar diferentes _backends_ de persistencia (por ejemplo, SQLite o Postgres).\n",
    "\n",
    "Así mismo, a partir de ahora, en vez de usar la función `.invoke` que es síncrona, usaremos la versión `.ainvoke` que es asíncrona y más acorde a un caso de uso real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory and create app\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos crear una configuración que pasemos al _runnable_ cada vez que interactuemos con el modelo. Esta configuración contiene información que no es parte de la entrada directamente, pero que sigue siendo útil. En este caso, queremos incluir un `thread_id` para identificar la conversación. Esto se ve así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Bob! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos permite soportar múltiples hilos de conversación con una sola aplicación, un requisito común cuando tu aplicación tiene múltiples usuarios.\n",
    "\n",
    "Luego podemos invocar la aplicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You mentioned that your name is Bob. How can I help you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que si continuamos la conversación preservando el `thread_id` ahora el chatbot tiene memoria de la misma sin necesidad de que pasemos en la llamada todo el historial de la conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You said your name is Bob. How can I assist you further?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡Genial!** Nuestro chatbot ahora recuerda cosas sobre nosotros. Si cambiamos la configuración para referenciar un `thread_id` diferente, podemos ver que comienza la conversación desde cero. Es decir, si cambianos el valor del `thread_id` estaríamos realmente creando una conversación nueva, y por tanto el historial de la anterior realmente no existe y el chatbot ya no recuerda nuestro nombre.\n",
    "\n",
    "Sin embargo, siempre podemos volver a la conversación original (ya que la estamos guardando en una base de datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal data about individuals unless it has been shared with me during our conversation. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plantillas de _prompts_\n",
    "\n",
    "Un _prompt_ En este contexto, un prompt es un conjunto de instrucciones o información que se proporciona a un modelo de lenguaje (LLM) para guiar su respuesta. Los _prompts_ pueden incluir mensajes del usuario, mensajes del sistema con instrucciones específicas, o cualquier otra información relevante que ayude al modelo a generar una respuesta adecuada. Por ejemplo, en un chatbot, un _prompt_ podría incluir el historial de la conversación y un mensaje del sistema que le indique al modelo cómo debe comportarse o qué tono debe usar. Esto ayuda a que el modelo entienda mejor el contexto y genere respuestas más coherentes y útiles.\n",
    "\n",
    "El escribir _prompts_ es tanto un arte como una ciencia, muy basada en el ensayo y el error, y totalmente dependiente de la naturaleza, tipo y versión del modelo de Inteligencia Artificial que estemos utilizando. Tanto es así que existe una disciplina al respecto llamada **_Prompt Engineering_**.\n",
    "\n",
    "Así, surgen las plantillas de _prompts_, unas construcciones que ayudan a convertir la información cruda del usuario en un formato con el que el modelo de lenguaje (LLM) pueda trabajar. En este caso, la entrada cruda del usuario es solo un mensaje, que estamos pasando al LLM. Ahora hagamos esto un poco más complicado. Primero, agreguemos un mensaje del sistema con algunas instrucciones personalizadas (pero aún tomando mensajes como entrada). Luego, agregaremos más entradas además de los mensajes.\n",
    "\n",
    "Para agregar un mensaje del sistema, crearemos una `ChatPromptTemplate`. Utilizaremos `MessagesPlaceholder` para pasar todos los mensajes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos actualizar nuestra aplicación para incorporar esta plantilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "async def call_model_with_prompt(state: MessagesState):\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke(state)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow_pirate = StateGraph(state_schema=MessagesState)\n",
    "workflow_pirate.add_edge(START, \"model\")\n",
    "workflow_pirate.add_node(\"model\", call_model_with_prompt)\n",
    "app_pirate = workflow_pirate.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un nuevo hilo de conversación para este ejemplo e invocamos la aplicación de la misma manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy, Jim! Welcome aboard! What be bringin' ye to these here waters today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app_pirate.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que mantenemos la memoria de la conversación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ye be Jim, if me memory serves correct! What can I do fer ye today, matey?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "\n",
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app_pirate.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¡Genial!** Ahora hagamos nuestro _prompt_ un poco más complicado. Supongamos que la plantilla del _prompt_ ahora se ve algo así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos añadido una nueva entrada de idioma (`language`) al _prompt_. Nuestra aplicación ahora tiene dos parámetros: los mensajes de entrada y el idioma. Debemos actualizar el estado de nuestra aplicación para reflejar esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "async def call_model_with_parameter(state: State):\n",
    "    chain = prompt | model\n",
    "    response = await chain.ainvoke(state)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow_with_parameter = StateGraph(state_schema=State)\n",
    "workflow_with_parameter.add_edge(START, \"model\")\n",
    "workflow_with_parameter.add_node(\"model\", call_model_with_parameter)\n",
    "app_with_parameter = workflow_with_parameter.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probarlo haciendo que nuestro chatbot reciba un mensaje en Inglés, pero nos conteste en Español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola, Bob! ¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app_with_parameter.ainvoke(\n",
    "    {\n",
    "        \"messages\": input_messages, \n",
    "        \"language\": language\n",
    "    },\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la memoria preserva el estado del idioma que pasamos, con lo cual no hay que continuar diciéndole que nos hable en Español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "\n",
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app_with_parameter.ainvoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestión del Historial de Conversaciones\n",
    "\n",
    "Un concepto importante a entender al construir chatbots es cómo gestionar el historial de conversaciones. Si no se gestiona, la lista de mensajes crecerá sin límites y potencialmente desbordará la ventana de contexto del modelo de lenguaje (LLM). Recordemos que los LLMs tienen límites en el número de letras (tokens) que podemos suministrar en las llamadas que realizamos, y por tanto, dentro del propio _prompt_. Así, es importante añadir un paso que limite el tamaño de los mensajes que estás pasando.\n",
    "\n",
    "Es importante hacer esto **antes** de la plantilla del _prompt_ pero **después** de cargar los mensajes anteriores del historial de mensajes.\n",
    "\n",
    "Podemos hacer esto añadiendo un paso simple delante del _prompt_ que modifique la clave de los mensajes adecuadamente, y luego envolver esa nueva cadena (`string`) en la clase de historial de mensajes.\n",
    "\n",
    "LangChain viene con algunos ayudantes incorporados para gestionar una lista de mensajes. En este caso, usaremos el ayudante `trim_messages` para reducir la cantidad de mensajes que estamos enviando al modelo. El `trimmer` nos permite especificar cuántos tokens queremos conservar, junto con otros parámetros como si queremos mantener siempre el mensaje del sistema y si permitimos mensajes parciales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages, ToolMessage\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    "    end_on=(\"human\", \"tool\"),\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usarlo en nuestra cadena (`string`), solo necesitamos ejecutar el `trimmer` antes de pasar la entrada de mensajes a nuestro _prompt_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "async def call_model_with_prompt_and_history(state: State):\n",
    "    chain = prompt | model\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    response = await chain.ainvoke(\n",
    "        {\n",
    "            \"messages\": trimmed_messages, \n",
    "            \"language\": state[\"language\"]\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow_with_prompt_and_history = StateGraph(state_schema=State)\n",
    "workflow_with_prompt_and_history.add_edge(START, \"model\")\n",
    "workflow_with_prompt_and_history.add_node(\"model\", call_model_with_prompt_and_history)\n",
    "app_with_prompt_and_history = workflow_with_prompt_and_history.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, si intentamos preguntarle al modelo nuestro nombre, no lo sabrá ya que recortamos esa parte del historial de chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal information such as your name. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "\n",
    "query = \"What is my name?\"\n",
    "\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = await app_with_prompt_and_history.ainvoke(\n",
    "    {\n",
    "        \"messages\": input_messages, \n",
    "        \"language\": language\n",
    "    },\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero si preguntamos sobre información que está dentro de los últimos mensajes, lo recuerda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked \"What's 2 + 2?\"\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "\n",
    "query = \"What math problem did I ask?\"\n",
    "\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = await app_with_prompt_and_history.ainvoke(\n",
    "    {\n",
    "        \"messages\": input_messages, \n",
    "        \"language\": language\n",
    "    },\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Streaming_\n",
    "\n",
    "Ahora tenemos un chatbot funcional. Sin embargo, una consideración muy importante de la experiencia de usuario (UX, _User Experience_) para las aplicaciones de chatbot es el _streaming_. Los modelos de lenguaje (LLMs) a veces pueden tardar un tiempo en responder y, para mejorar la experiencia del usuario, una cosa que la mayoría de las aplicaciones hacen es transmitir cada token a medida que se genera. Esto permite al usuario ver el progreso tal como si la Inteligencia Artificial le estuviera escribiendo.\n",
    "\n",
    "**¡En realidad es muy fácil hacer esto!**\n",
    "\n",
    "Por defecto, `.stream` en nuestra aplicación LangGraph transmite los pasos de la aplicación; en este caso, el único paso de la respuesta del modelo. El configurar `stream_mode=\"messages\"` nos permite transmitir tokens de salida en su lugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||Hi| Todd|!| Sure|,| here|’s| a| joke| for| you|:\n",
      "\n",
      "|Why| don't| scientists| trust| atoms|?\n",
      "\n",
      "|Because| they| make| up| everything|!||"
     ]
    }
   ],
   "source": [
    "def call_model_with_prompt_and_history_stream(state: State):\n",
    "    chain = prompt | model\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    response = chain.invoke(\n",
    "        {\n",
    "            \"messages\": trimmed_messages, \n",
    "            \"language\": state[\"language\"]\n",
    "        }\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "workflow_with_prompt_and_history_stream = StateGraph(state_schema=State)\n",
    "workflow_with_prompt_and_history_stream.add_edge(START, \"model\")\n",
    "workflow_with_prompt_and_history_stream.add_node(\"model\", call_model_with_prompt_and_history_stream)\n",
    "app_with_prompt_and_history_stream = workflow_with_prompt_and_history_stream.compile(checkpointer=MemorySaver())\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"7\"}}\n",
    "\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app_with_prompt_and_history_stream.stream(\n",
    "    {\n",
    "        \"messages\": input_messages, \n",
    "        \"language\": language\n",
    "    },\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
